{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析数据集\n",
    "\n",
    "数据集名为`news-commentary-v6`，包含……五种语言的文本数据。打印数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musharraf's Last Act?\n",
      "Desperate to hold onto power, Pervez Musharraf has discarded Pakistan's constitutional framework and declared a state of emergency.\n",
      "His goal?\n",
      "To stifle the independent judiciary \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"./data\")\n",
    "DATASET_NAME = 'news-commentary-v6'\n",
    "lang = 'en'\n",
    "\n",
    "with open(DATA_DIR / f'{DATASET_NAME}.{lang}') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到数据组织为每行一句话，我们可以将整个文件作为一整个字符串进行分词器的训练和分词。\n",
    "\n",
    "### 训练分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordPiece with vocab size 1000\n",
      "\n",
      "\n",
      "\n",
      "Training BPE with vocab size 1000\n",
      "\n",
      "\n",
      "\n",
      "Training Unigram with vocab size 1000\n",
      "\n",
      "\n",
      "Training WordPiece with vocab size 3000\n",
      "\n",
      "\n",
      "\n",
      "Training BPE with vocab size 3000\n",
      "\n",
      "\n",
      "\n",
      "Training Unigram with vocab size 3000\n",
      "\n",
      "\n",
      "Training WordPiece with vocab size 5000\n",
      "\n",
      "\n",
      "\n",
      "Training BPE with vocab size 5000\n",
      "\n",
      "\n",
      "\n",
      "Training Unigram with vocab size 5000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece, BPE, Unigram\n",
    "from tokenizers.trainers import WordPieceTrainer, BpeTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "VOCAB_SIZE = [1000, 3000, 5000]\n",
    "NAME_TEMPLATE = \"{model}-{vocab_size}.json\"\n",
    "TOKENIZER_DIR = Path(\"./tokenizers\")\n",
    "if not TOKENIZER_DIR.exists():\n",
    "    TOKENIZER_DIR.mkdir()\n",
    "models = [WordPiece, BPE, Unigram]\n",
    "trainers = [WordPieceTrainer, BpeTrainer, UnigramTrainer]\n",
    "\n",
    "\n",
    "for vocab_size, (model, trainer) in product(VOCAB_SIZE, zip(models, trainers)):\n",
    "    print(f\"Training {model.__name__} with vocab size {vocab_size}\")\n",
    "    tokenizer = Tokenizer(model())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = trainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\"])\n",
    "    tokenizer.train([str(DATA_DIR / f'{DATASET_NAME}.{lang}')], trainer)\n",
    "    file_name = NAME_TEMPLATE.format(model=model.__name__, vocab_size=vocab_size)\n",
    "    tokenizer.save(str(TOKENIZER_DIR / file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算压缩率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece-1000.json: 2.73\n",
      "BPE-1000.json: 2.99\n",
      "Unigram-1000.json: 2.69\n",
      "WordPiece-3000.json: 3.76\n",
      "BPE-3000.json: 3.92\n",
      "Unigram-3000.json: 3.48\n",
      "WordPiece-5000.json: 4.23\n",
      "BPE-5000.json: 4.35\n",
      "Unigram-5000.json: 3.75\n"
     ]
    }
   ],
   "source": [
    "def compute_compression_ratio(text, tokenizer):\n",
    "    num_utf8_bytes = len(text.encode('utf-8'))\n",
    "    encoded_ids = tokenizer.encode(text)\n",
    "    num_encoded_ids = len(encoded_ids.ids)\n",
    "    return num_utf8_bytes / num_encoded_ids\n",
    "\n",
    "for vocab_size, (model, trainer) in product(VOCAB_SIZE, zip(models, trainers)):\n",
    "    file_name = NAME_TEMPLATE.format(model=model.__name__, vocab_size=vocab_size)\n",
    "    tokenizer = Tokenizer.from_file(str(TOKENIZER_DIR / file_name))\n",
    "    ratio = compute_compression_ratio(text, tokenizer)\n",
    "    print(f\"{file_name}: {ratio:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
